{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torcheval.metrics import Mean, MulticlassAccuracy\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "\n",
    "from aum_ranking import assign_threshold_samples, ThresholdSamplesDataset, AUM, compute_aum_threshold, flag_mislabeled_examples, combine_mislabeled_examples\n",
    "from models import ResNet32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how to corrupt the training dataset labels\n",
    "\n",
    "def assign_corrupted_samples(dataset, fraction):\n",
    "    corrupted_flags = np.zeros((len(dataset),), dtype=np.uint8)\n",
    "    num_corrupted_samples = round(fraction * len(dataset))\n",
    "    corrupted_flags[:num_corrupted_samples] = 1\n",
    "    np.random.shuffle(corrupted_flags)\n",
    "\n",
    "    real_labels = np.array([y for x, y in dataset])\n",
    "    noise = np.random.randint(1, len(dataset.classes), size=len(dataset))\n",
    "    fake_labels = (real_labels + noise) % len(dataset.classes)\n",
    "    corrupted_labels = np.where(corrupted_flags, fake_labels, real_labels)\n",
    "    return corrupted_labels, corrupted_flags\n",
    "\n",
    "\n",
    "class CorruptedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A Dataset wrapper which synthetically mislabels some of the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, labels):\n",
    "        self.dataset = dataset\n",
    "        self.labels = labels\n",
    "        self.classes = dataset.classes\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.dataset[index]\n",
    "        return x, self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "def corrupt_dataset(dataset, fraction):\n",
    "    corrupted_labels, corrupted_flags = assign_corrupted_samples(dataset, fraction)\n",
    "    return CorruptedDataset(dataset, corrupted_labels), corrupted_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets\n",
    "\n",
    "pixel_means = torch.tensor([0.4914, 0.4822, 0.4465])\n",
    "pixel_stds = torch.tensor([0.2470, 0.2435, 0.2616])\n",
    "\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=pixel_means, std=pixel_stds)\n",
    "])\n",
    "\n",
    "# This will be used later for visualizing images\n",
    "unprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Normalize(mean=-pixel_means / pixel_stds, std=1.0 / pixel_stds),\n",
    "    torchvision.transforms.ToPILImage()\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=torchvision.transforms.Compose([\n",
    "    preprocess,\n",
    "    torchvision.transforms.Pad(4),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomCrop(32)\n",
    "]))\n",
    "\n",
    "train_dataset, corrupted_flags = corrupt_dataset(train_dataset, 0.2)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign two sets of threshold samples\n",
    "\n",
    "threshold_sample_flags_1, threshold_sample_flags_2 = assign_threshold_samples(num_examples=len(train_dataset), num_classes=len(train_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
    "print(f\"Number of samples: {len(train_dataset)}\")\n",
    "print(f\"Number of threshold samples (first pass): {threshold_sample_flags_1.sum()}\")\n",
    "print(f\"Number of threshold samples (second pass): {threshold_sample_flags_2.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for visualizing predictions\n",
    "\n",
    "def predict_with_probs(model, x):\n",
    "    \"\"\"\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    \"\"\"\n",
    "    x = x.to(device)\n",
    "    logits = model(x)\n",
    "    output = F.softmax(logits, dim=1)\n",
    "    probs, preds = torch.max(output, 1)\n",
    "    probs = probs.cpu().numpy()\n",
    "    preds = preds.cpu().numpy()\n",
    "    return preds, probs\n",
    "\n",
    "\n",
    "def plot_classes_preds(model, x, y, classes):\n",
    "    \"\"\"\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        preds, probs = predict_with_probs(model, x)\n",
    "    # Plot the images in the batch, along with predicted and true labels\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(10, 3))\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        ax.imshow(unprocess(x[i]))\n",
    "        ax.set_title(\n",
    "            \"{0}, {1:.1%}\\n(actual: {2})\".format(\n",
    "                classes[preds[i]],\n",
    "                probs[i],\n",
    "                classes[y[i]]),\n",
    "            color=(\"green\" if preds[i]==y[i].item() else \"red\")\n",
    "        )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, progress, model, loss_fn, optimizer, epoch, epochs, writer, loss_metric, accuracy_metric, aum):\n",
    "    progress.reset()\n",
    "    progress.desc = f\"Epoch {epoch+1}/{epochs}\"\n",
    "\n",
    "    model.train()\n",
    "    for x, y, indexes in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_metric.update(loss.detach())\n",
    "        accuracy_metric.update(pred, y)\n",
    "\n",
    "        progress.set_postfix_str(\n",
    "            f\"loss={loss_metric.compute().item():.4f}, accuracy={accuracy_metric.compute().item():.2%}\",\n",
    "            refresh=False\n",
    "        )\n",
    "\n",
    "        aum.update(pred, y, indexes)\n",
    "\n",
    "        progress.update()\n",
    "\n",
    "    writer.add_scalar(\"loss/train\", scalar_value=loss_metric.compute(), global_step=epoch)\n",
    "    loss_metric.reset()\n",
    "    writer.add_scalar(\"accuracy/train\", scalar_value=accuracy_metric.compute(), global_step=epoch)\n",
    "    accuracy_metric.reset()\n",
    "\n",
    "    progress.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, model, loss_fn, epoch, writer, loss_metric, accuracy_metric, classes):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            loss_metric.update(loss)\n",
    "            accuracy_metric.update(pred, y)\n",
    "    \n",
    "    writer.add_scalar(\"loss/test\", scalar_value=loss_metric.compute(), global_step=epoch)\n",
    "    loss_metric.reset()\n",
    "    writer.add_scalar(\"accuracy/test\", scalar_value=accuracy_metric.compute(), global_step=epoch)\n",
    "    accuracy_metric.reset()\n",
    "\n",
    "    x, y = zip(*random.choices(loader.dataset, k=4))\n",
    "    x = torch.stack(x)\n",
    "    y = torch.tensor(y)\n",
    "    writer.add_figure(\"predictions\", plot_classes_preds(model, x, y, classes), global_step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_dataset(model, loader):\n",
    "    preds = np.empty(len(loader.dataset), dtype=np.int32)\n",
    "    probs = np.empty(len(loader.dataset), dtype=np.int32)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y, indexes in loader:\n",
    "            pred, prob = predict_with_probs(model, x)\n",
    "            preds[indexes] = pred\n",
    "            probs[indexes] = prob\n",
    "\n",
    "    return preds, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_run(pass_index, threshold_dataset, test_dataset, epochs, aum):\n",
    "    # Make data loaders\n",
    "    batch_size = 64\n",
    "    threshold_loader = DataLoader(threshold_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Build model\n",
    "    model = ResNet32(num_classes=len(threshold_dataset.classes)).to(device)\n",
    "\n",
    "    # Set loss function and optimizer\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4, momentum=0.9, nesterov=True)\n",
    "\n",
    "    # Make TensorBoard writer\n",
    "    now = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
    "    log_dir = os.path.join(\"runs\", \"find_mislabeled_data\", f\"{now}_pass_{pass_index}\")\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    # Define metrics\n",
    "    loss_metric = Mean(device=device)\n",
    "    accuracy_metric = MulticlassAccuracy(device=device)\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training model\")\n",
    "    print(\"-------------------------------\")\n",
    "    with tqdm(total=len(threshold_loader)) as train_progress:\n",
    "        for epoch in range(epochs):\n",
    "            train(threshold_loader, train_progress, model, loss_fn, optimizer, epoch, epochs, writer, loss_metric, accuracy_metric, aum)\n",
    "            test(test_loader, model, loss_fn, epoch, writer, loss_metric, accuracy_metric, classes=threshold_loader.dataset.classes)\n",
    "    suggested_labels, suggested_label_probs = predict_on_dataset(model, threshold_loader)\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return model, suggested_labels, suggested_label_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mislabeled_examples_pass(pass_index, threshold_sample_flags, train_dataset, test_dataset):\n",
    "    print(f\"Performing pass {pass_index}\")\n",
    "\n",
    "    # Make threshold samples dataset\n",
    "    threshold_dataset = ThresholdSamplesDataset(train_dataset, threshold_sample_flags)\n",
    "\n",
    "    # Create AUM calculator\n",
    "    aum = AUM(num_examples=len(threshold_dataset))\n",
    "\n",
    "    # Train a model to populate the margin values\n",
    "    epochs = 150\n",
    "    model, suggested_labels, suggested_label_probs = training_run(pass_index, threshold_dataset, test_dataset, epochs, aum)\n",
    "\n",
    "    # Compute AUM values\n",
    "    aum_values = aum.compute(epochs).numpy()\n",
    "    print(f\"AUM values: {aum_values.shape}, {aum_values.dtype}\")\n",
    "    print(f\"mean: {np.mean(aum_values):.4f}, min: {np.min(aum_values):.4f}, max: {np.max(aum_values):.4f}, std: {np.std(aum_values):.4f}\")\n",
    "\n",
    "    # Compute AUM threshold\n",
    "    aum_threshold = compute_aum_threshold(aum_values, threshold_sample_flags)\n",
    "    print(f\"AUM threshold: {aum_threshold}\")\n",
    "\n",
    "    # Flag (potentially) mislabeled examples\n",
    "    mislabeled_example_flags = flag_mislabeled_examples(aum_values, threshold_sample_flags, aum_threshold)\n",
    "    print(f\"Potentially mislabeled examples: {np.sum(mislabeled_example_flags)}\")\n",
    "    print(f\"Finished pass {pass_index}\")\n",
    "    print(\"===============================\")\n",
    "    return mislabeled_example_flags, suggested_labels, suggested_label_probs\n",
    "\n",
    "\n",
    "def combine_suggested_labels(suggested_labels_1, suggested_label_probs_1, suggested_labels_2, suggested_label_probs_2):\n",
    "    choose_1 = suggested_label_probs_1 >= suggested_label_probs_2\n",
    "    suggested_labels = np.where(choose_1, suggested_labels_1, suggested_labels_2)\n",
    "    return suggested_labels\n",
    "\n",
    "\n",
    "def find_mislabeled_examples(train_dataset, test_dataset, threshold_sample_flags_1, threshold_sample_flags_2):\n",
    "    mislabeled_example_flags_1, suggested_labels_1, suggested_label_probs_1 = find_mislabeled_examples_pass(1, threshold_sample_flags_1, train_dataset, test_dataset)\n",
    "    mislabeled_example_flags_2, suggested_labels_2, suggested_label_probs_2 = find_mislabeled_examples_pass(2, threshold_sample_flags_2, train_dataset, test_dataset)\n",
    "    mislabeled_example_flags = combine_mislabeled_examples(mislabeled_example_flags_1, mislabeled_example_flags_2)\n",
    "    suggested_labels = combine_suggested_labels(suggested_labels_1, suggested_label_probs_1, suggested_labels_2, suggested_label_probs_2)\n",
    "    return mislabeled_example_flags, suggested_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabeled_example_flags, suggested_labels = find_mislabeled_examples(train_dataset, test_dataset, threshold_sample_flags_1, threshold_sample_flags_2)\n",
    "print(f\"Mislabeled example flags: {mislabeled_example_flags.shape}, {mislabeled_example_flags.dtype}\")\n",
    "print(f\"Potentially mislabeled examples: {np.sum(mislabeled_example_flags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mislabeled_examples(dataset, mislabeled_example_flags, suggested_labels):\n",
    "    (mislabeled_example_indexes,) = np.nonzero(mislabeled_example_flags)\n",
    "    indexes = np.random.choice(mislabeled_example_indexes, 6)\n",
    "    fig, axs = plt.subplots(1, 6, figsize=(15, 3.6))\n",
    "    fig.suptitle(\"Potentially mislabeled examples\")\n",
    "    for i, ax in zip(indexes, axs):\n",
    "        x, y = dataset[i]\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        ax.imshow(unprocess(x))\n",
    "        ax.set_title(f\"{i}\\n{dataset.classes[y]}\\n(suggested: {dataset.classes[suggested_labels[i]]})\")\n",
    "\n",
    "\n",
    "plot_mislabeled_examples(train_dataset, mislabeled_example_flags, suggested_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabeled_true_positives = np.count_nonzero(mislabeled_example_flags & corrupted_flags)\n",
    "mislabeled_false_positives = np.count_nonzero(mislabeled_example_flags & ~corrupted_flags)\n",
    "mislabeled_false_negatives = np.count_nonzero(~mislabeled_example_flags & corrupted_flags)\n",
    "\n",
    "precision = mislabeled_true_positives / (mislabeled_true_positives + mislabeled_false_positives)\n",
    "recall = mislabeled_true_positives / (mislabeled_true_positives + mislabeled_false_negatives)\n",
    "\n",
    "print(f\"Mislabeled example identification\")\n",
    "print(f\"precision: {precision:.2%}\")\n",
    "print(f\"recall:    {recall:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
