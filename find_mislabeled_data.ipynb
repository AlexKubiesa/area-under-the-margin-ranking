{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torcheval.metrics import Mean, MulticlassAccuracy\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "\n",
    "from aum_ranking import assign_threshold_samples, ThresholdSamplesDataset, AUM, compute_aum_threshold, flag_mislabeled_examples\n",
    "from models import ResNet32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download datasets\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make threshold samples dataset\n",
    "\n",
    "threshold_sample_flags = assign_threshold_samples(num_examples=len(train_dataset), num_classes=len(train_dataset.classes))\n",
    "threshold_dataset = ThresholdSamplesDataset(train_dataset, threshold_sample_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
    "print(f\"Number of samples: {len(threshold_dataset)}\")\n",
    "print(f\"Number of threshold samples: {sum(1 for x, y in threshold_dataset if y == len(train_dataset.classes))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data loaders\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "threshold_loader = DataLoader(threshold_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "x, y = next(iter(test_loader))\n",
    "print(f\"x: {x.shape}, {x.dtype}\")\n",
    "print(f\"y: {y.shape}, {y.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "model = ResNet32(num_classes=len(threshold_dataset.classes)).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set loss function and optimizer\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make TensorBoard writer\n",
    "\n",
    "now = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
    "log_dir = os.path.join(\"runs\", \"find_mislabeled_data\", now)\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "\n",
    "loss_metric = Mean(device=device)\n",
    "accuracy_metric = MulticlassAccuracy(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for visualizing predictions\n",
    "\n",
    "to_pil_image = torchvision.transforms.ToPILImage()\n",
    "\n",
    "\n",
    "def predict_with_probs(model, x):\n",
    "    \"\"\"\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    \"\"\"\n",
    "    logits = model(x)\n",
    "    output = F.softmax(logits, dim=1)\n",
    "    probs, preds = torch.max(output, 1)\n",
    "    return preds, probs\n",
    "\n",
    "\n",
    "def plot_classes_preds(model, x, y, classes):\n",
    "    \"\"\"\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        preds, probs = predict_with_probs(model, x.to(device))\n",
    "    preds = preds.cpu().numpy()\n",
    "    probs = probs.cpu().numpy()\n",
    "    # Plot the images in the batch, along with predicted and true labels\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(10, 3))\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        ax.imshow(to_pil_image(x[i]))\n",
    "        ax.set_title(\n",
    "            \"{0}, {1:.1%}\\n(actual: {2})\".format(\n",
    "                classes[preds[i]],\n",
    "                probs[i],\n",
    "                classes[y[i]]),\n",
    "            color=(\"green\" if preds[i]==y[i].item() else \"red\")\n",
    "        )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AUM calculator\n",
    "\n",
    "aum = AUM(num_examples=len(threshold_dataset), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, model, loss_fn, optimizer, epoch, writer, loss_metric, accuracy_metric, aum):\n",
    "    model.train()\n",
    "    with tqdm(loader) as progress:\n",
    "        for batch, (x, y) in enumerate(progress):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_metric.update(loss.detach())\n",
    "            accuracy_metric.update(pred, y)\n",
    "\n",
    "            progress.set_postfix_str(\n",
    "                f\"loss={loss_metric.compute().item():.4f}, accuracy={accuracy_metric.compute().item():.2%}\",\n",
    "                refresh=False\n",
    "            )\n",
    "\n",
    "            aum.update(pred, y, start=loader.batch_size * batch)\n",
    "\n",
    "    writer.add_scalar(\"loss/train\", scalar_value=loss_metric.compute(), global_step=epoch)\n",
    "    loss_metric.reset()\n",
    "    writer.add_scalar(\"accuracy/train\", scalar_value=accuracy_metric.compute(), global_step=epoch)\n",
    "    accuracy_metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, model, loss_fn, epoch, writer, loss_metric, accuracy_metric, classes):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            loss_metric.update(loss)\n",
    "            accuracy_metric.update(pred, y)\n",
    "    \n",
    "    print(f\"test_loss={loss_metric.compute().item():.4f}, test_accuracy={accuracy_metric.compute().item():.2%}\")\n",
    "\n",
    "    writer.add_scalar(\"loss/test\", scalar_value=loss_metric.compute(), global_step=epoch)\n",
    "    loss_metric.reset()\n",
    "    writer.add_scalar(\"accuracy/test\", scalar_value=accuracy_metric.compute(), global_step=epoch)\n",
    "    accuracy_metric.reset()\n",
    "\n",
    "    x, y = zip(*random.choices(loader.dataset, k=4))\n",
    "    x = torch.stack(x)\n",
    "    y = torch.tensor(y)\n",
    "    writer.add_figure(\"predictions\", plot_classes_preds(model, x, y, classes), global_step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    print(\"-------------------------------\")\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train(threshold_loader, model, loss_fn, optimizer, epoch, writer, loss_metric, accuracy_metric, aum)\n",
    "    test(test_loader, model, loss_fn, epoch, writer, loss_metric, accuracy_metric, classes=threshold_loader.dataset.classes)\n",
    "print(\"-------------------------------\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aum_values = aum.compute(epochs).cpu().numpy()\n",
    "\n",
    "print(f\"AUM values: {aum_values.shape}, {aum_values.dtype}\")\n",
    "print(f\"mean: {np.mean(aum_values):.4f}, min: {np.min(aum_values):.4f}, max: {np.max(aum_values):.4f}, std: {np.std(aum_values):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AUM threshold\n",
    "\n",
    "aum_threshold = compute_aum_threshold(aum_values, threshold_sample_flags)\n",
    "print(f\"AUM threshold: {aum_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag (potentially) mislabeled examples\n",
    "\n",
    "mislabeled_example_flags = flag_mislabeled_examples(aum_values, threshold_sample_flags, aum_threshold)\n",
    "print(f\"Mislabeled example flags: {mislabeled_example_flags.shape}, {mislabeled_example_flags.dtype}\")\n",
    "print(f\"Potentially mislabeled examples: {np.sum(mislabeled_example_flags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mislabeled_examples(model, dataset, mislabeled_example_flags, classes):\n",
    "    (mislabeled_example_indexes,) = np.nonzero(mislabeled_example_flags)\n",
    "    indexes = np.random.choice(mislabeled_example_indexes, 6)\n",
    "    fig, axs = plt.subplots(1, 6, figsize=(15, 3.8))\n",
    "    fig.suptitle(\"Potentially mislabeled examples\")\n",
    "    for i, ax in enumerate(axs):\n",
    "        x, y = dataset[indexes[i]]\n",
    "        with torch.no_grad():\n",
    "            pred = model(x.to(device)[None])[0].argmax(0)\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        ax.imshow(to_pil_image(x))\n",
    "        ax.set_title(f\"{classes[y]}\\nAUM={aum_values[indexes[i]]:.4f}\\n(predicted: {classes[pred]})\")\n",
    "\n",
    "\n",
    "plot_mislabeled_examples(model, threshold_dataset, mislabeled_example_flags, threshold_dataset.classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
